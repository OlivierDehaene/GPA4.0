{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Your Own Visualizations!\n",
    "Instructions:\n",
    "1. Install tensor2tensor and train up a Transformer model following the instruction in the repository https://github.com/tensorflow/tensor2tensor.\n",
    "2. Update cell 3 to point to your checkpoint, it is currently set up to read from the default checkpoint location that would be created from following the instructions above.\n",
    "3. If you used custom hyper parameters then update cell 4.\n",
    "4. Run the notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/anaconda3/envs/1.5_cpu/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensor2tensor import problems\n",
    "from tensor2tensor.bin import t2t_decoder  # To register the hparams set\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "from tensor2tensor.visualization import attention\n",
    "from tensor2tensor.visualization import visualization\n",
    "from tensor2tensor.utils import usr_dir\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min'\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PUT THE MODEL YOU WANT TO LOAD HERE!\n",
    "CHECKPOINT = \"../checkpoints/word_to_phonetic/transformer-transformer_base_single_gpu-fr-best_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HParams\n",
    "problem_name = 'word_to_phonetic'\n",
    "data_dir = \"../data_dir/fr\"\n",
    "model_name = \"transformer\"\n",
    "hparams_set = \"transformer_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Importing user module submodule from path /home/olivier/Bureau/Transformer_test\n"
     ]
    }
   ],
   "source": [
    "#Adding word to phonetic problem to the problem list\n",
    "submodule_dir = \"../submodule\"\n",
    "usr_dir.import_usr_dir(submodule_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_258_512.bottom\n",
      "INFO:tensorflow:Transforming 'targets' with symbol_modality_258_512.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "WARNING:tensorflow:From /home/olivier/anaconda3/envs/1.5_cpu/lib/python3.5/site-packages/tensor2tensor/layers/common_layers.py:513: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_258_512.top\n",
      "WARNING:tensorflow:From /home/olivier/anaconda3/envs/1.5_cpu/lib/python3.5/site-packages/tensor2tensor/layers/common_layers.py:1708: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Beam Decoding with beam size 5\n",
      "WARNING:tensorflow:From /home/olivier/anaconda3/envs/1.5_cpu/lib/python3.5/site-packages/tensor2tensor/utils/beam_search.py:93: calling reduce_logsumexp (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "visualizer = visualization.AttentionVisualizer(hparams_set, model_name, data_dir, problem_name, beam_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ../checkpoints/word_to_phonetic/transformer-transformer_base_single_gpu-fr-best_model/model.ckpt-9001\n"
     ]
    }
   ],
   "source": [
    "tf.Variable(0, dtype=tf.int64, trainable=False, name='global_step')\n",
    "\n",
    "sess = tf.train.MonitoredTrainingSession(\n",
    "    checkpoint_dir=CHECKPOINT,\n",
    "    save_summaries_secs=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#input example\n",
    "input_word = \"test\"\n",
    "_, inp_text, out_text, att_mats = visualizer.get_vis_data_from_string(sess, input_word)\n",
    "inp_text = [str(c, 'Latin-1') for c in inp_text] #Decodes Latin-1 because of Frenche and Spanish special chars\n",
    "out_text = [str(c, 'Latin-1') for c in out_text]\n",
    "\n",
    "#Uncomment to see all attention heads and have a better understanding of what's happening inside the model\n",
    "#attention.show(inp_text, out_text, *att_mats)\n",
    "\n",
    "#Removes both padding and \"end of sequence\" markers\n",
    "inp_text = [v for v in inp_text if v != '<EOS>']\n",
    "out_text = [v for v in out_text if v != '<EOS>' and v != '<pad>']\n",
    "\n",
    "#Gets layes 0 and 4 of the \"inp_out\" matrices\n",
    "att_matrices = np.array(attention._get_attention(inp_text, out_text, *att_mats)[\"inp_out\"][\"att\"])[np.array([0,4,5]),:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[118]],\n",
       "\n",
       "        [[ 71]],\n",
       "\n",
       "        [[117]],\n",
       "\n",
       "        [[118]],\n",
       "\n",
       "        [[  1]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizer.encode(\"\".join(out_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(matrix):\n",
    "    \"\"\"\n",
    "        input: a numpy matrix\n",
    "        return: matrix with 0 mean and 1 std\n",
    "    \"\"\"\n",
    "    return (matrix - np.mean(matrix))/np.std(matrix)\n",
    "\n",
    "#Sum all attention heads\n",
    "sum_all_head = np.sum(att_matrices, axis=1)\n",
    "\n",
    "#Sum layers 0 and 4\n",
    "sum_all_layers = normalize(np.sum(sum_all_head,axis=0)[:len(out_text), :len(inp_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hu-y', 'l-l', 'u-y', 'l-l', 'a-E', 'i-#', 'e-E', 'nt-#']\n"
     ]
    }
   ],
   "source": [
    "#Base threshold\n",
    "#fr : 0.75\n",
    "#es : 0.4\n",
    "if len(out_text) > 4:\n",
    "    threshold = 0.75\n",
    "else : \n",
    "    threshold = 0\n",
    "\n",
    "#While we have too many silent_letters detected\n",
    "while(True):\n",
    "    #Gets the silent_letters indices\n",
    "    #We consider that a letter is silent if its attention value is below mean attention + threshold * std attention\n",
    "    silent_letters_idx = [i for i, idx in enumerate(np.argmax(sum_all_layers, axis = 0)) \n",
    "                          if sum_all_layers[idx, i] < np.mean(sum_all_layers[idx,:]) \n",
    "                          + threshold*np.std(sum_all_layers[idx,:])]\n",
    "    #Reduces threshold if too many silent letters are detected\n",
    "    #Can happen in french when we have 3 lettres graphemes\n",
    "    if len(silent_letters_idx) > 1/3 * len(inp_text):\n",
    "        threshold -= 0.1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#Creates the phoneme attribution list\n",
    "phon_list = np.array(out_text)[np.argmax(sum_all_layers, axis = 0)]\n",
    "phon_list[silent_letters_idx] = \"#\" #\"#\" is our encoding for silent letters\n",
    "phon_list = phon_list.tolist() #needed for the += just below\n",
    "\n",
    "#Checks if all the phonemes are attributed and if they are only present the correct number of time in the list\n",
    "#If not, the phoneme is concatenated to its most probable neighbor\n",
    "#and the least probable phoneme is replaced by a silent letter (this can happen for small datasets)\n",
    "for i, phon in enumerate(out_text):\n",
    "    if phon not in phon_list:\n",
    "        phon_list[np.argmax(sum_all_layers[i,:])] += phon\n",
    "    \n",
    "#     test = np.where(np.array(phon_list) == phon)[0]\n",
    "#     if len(test > 1):\n",
    "#         phon_list[np.max(test)] = \"%\"\n",
    "\n",
    "##NOT WORKING PROPERLY\n",
    "\n",
    "#Creates the g_p tupple list\n",
    "g_p = [(l, phon_list[i]) for i, l in enumerate(inp_text)]\n",
    "\n",
    "#Creates the final g_p mapping\n",
    "mapping = []\n",
    "for phon, letters in groupby(g_p, lambda x: x[1]):\n",
    "    graph = \"\".join([letter[0] for letter in letters])\n",
    "    mapping.append(graph + \"-\" + phon)\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.06525102 -0.31792047 -0.36558086 -0.46200795 -0.66098948] 0.6180536102139619\n",
      "[ 1.8992929   0.69367388 -0.41562538 -0.63287608 -0.66053605] 0.9941644975265498\n",
      "[ 3.0639282   0.43804662 -0.10846462 -0.36167091 -0.66071512] 1.344188215406739\n",
      "[ 3.3779973   0.40099458  0.10375085 -0.32120024 -0.65302955] 1.4436172856362397\n",
      "[ 2.85270214  0.20574147 -0.07796076 -0.22529724 -0.32006145] 1.1961228347479336\n",
      "[ 2.27176321 -0.32876776 -0.54840714 -0.66066426 -0.66095732] 1.1350791797926991\n",
      "[ 0.501696   -0.29333061 -0.55300606 -0.58932894 -0.63060675] 0.4240167590987156\n",
      "[ 0.59766129 -0.2974547  -0.6410225  -0.64869399 -0.65555726] 0.4828867736564058\n",
      "[ 0.05114794 -0.38156322 -0.64586064 -0.64863634 -0.6608068 ] 0.2748945166468728\n",
      "[-0.29617655 -0.29777911 -0.56485272 -0.62207582 -0.65416272] 0.15777707881038736\n",
      "['hu-y', 'l-l', 'u-y', 'l-l', 'aie-E', 'nt-$']\n"
     ]
    }
   ],
   "source": [
    "def recurent_check(normalized_value_row, idx_row, idx_last_phon, out_text, j):\n",
    "    print(normalized_value_row, np.std(normalized_value_row))\n",
    "    if normalized_value_row[j] > 0.75*np.std(normalized_value_row):\n",
    "        if(idx_row[j] == 0 or idx_row[j] >= idx_last_phon):\n",
    "            return idx_row[j], out_text[idx_row[j]]\n",
    "        else:\n",
    "            return recurent_check(normalized_value_row, idx_row, idx_last_phon, out_text, j+1)\n",
    "    else:\n",
    "        return -1, \"$\"\n",
    "\n",
    "\n",
    "phon_list = []\n",
    "idx_last_phon = 0\n",
    "\n",
    "idx_sorted_phon_per_letter = np.argsort(-sum_all_layers, axis=0)[:,:].transpose()\n",
    "value_sorted_phon_per_letter = -np.sort(-sum_all_layers, axis=0)[:,:].transpose()\n",
    "\n",
    "for i, idx in enumerate(idx_sorted_phon_per_letter[:,0]):\n",
    "    normalized_value_row = value_sorted_phon_per_letter[i,:]\n",
    "    idx_row = idx_sorted_phon_per_letter[i,:]\n",
    "    \n",
    "    idx_phon, phon = recurent_check(normalized_value_row, idx_row, idx_last_phon, out_text, 0)\n",
    "    \n",
    "    phon_list.append(phon)\n",
    "    idx_last_phon = idx_phon\n",
    "    \n",
    "#Creates the g_p tupple list\n",
    "g_p = [(l, phon_list[i]) for i, l in enumerate(inp_text)]\n",
    "\n",
    "#Creates the final g_p mapping\n",
    "mapping = []\n",
    "for phon, letters in groupby(g_p, lambda x: x[1]):\n",
    "    graph = \"\".join([letter[0] for letter in letters])\n",
    "    mapping.append(graph + \"-\" + phon)\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h-$', 'u-y', 'l-l', 'u-y', 'l-l', 'a-E', 'ient-$']\n"
     ]
    }
   ],
   "source": [
    "g_p = []\n",
    "base_threshold = 2\n",
    "\n",
    "idx_sorted_letter_per_phon = np.argsort(-sum_all_layers, axis=1)[:,:]\n",
    "value_sorted_letter_per_phon = -np.sort(-sum_all_layers, axis=1)[:,:]\n",
    "\n",
    "for i, phon in enumerate(out_text):\n",
    "    normalized_value_row = value_sorted_letter_per_phon[i,:]\n",
    "    idx_row = idx_sorted_letter_per_phon[i,:]\n",
    "    threshold = base_threshold\n",
    "    while(True):\n",
    "        letters = np.array(inp_text)[idx_row[np.where(normalized_value_row > threshold * np.std(normalized_value_row))]]\n",
    "        if len(letters) == 0:\n",
    "            threshold -= 0.1\n",
    "        else:\n",
    "            break\n",
    "    for l in letters:\n",
    "        g_p.append((l, phon))\n",
    "        \n",
    "for i, l in enumerate(inp_text):\n",
    "    if i == len(g_p):\n",
    "        g_p.append((l, \"$\"))\n",
    "    elif g_p[i][0] != l:\n",
    "        g_p.insert(i, (l, \"$\"))\n",
    "\n",
    "mapping = []\n",
    "for phon, letters in groupby(g_p, lambda x: x[1]):\n",
    "    graph = \"\".join([letter[0] for letter in letters])\n",
    "    mapping.append(graph + \"-\" + phon)\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hu-y', 'l-l', 'u-y', 'l-l', 'a-E', 'i-$', 'e-E', 'nt-$']\n"
     ]
    }
   ],
   "source": [
    "phon_list = []\n",
    "idx_sorted_phon_per_letter = np.argsort(-sum_all_layers, axis=0)[:,:].transpose()\n",
    "value_sorted_phon_per_letter = -np.sort(-sum_all_layers, axis=0)[:,:].transpose()\n",
    "\n",
    "for i, idx in enumerate(idx_sorted_phon_per_letter[:,0]):\n",
    "    normalized_value_row = value_sorted_phon_per_letter[i,:]\n",
    "    idx_row = idx_sorted_phon_per_letter[i,:]\n",
    "    \n",
    "    phon = np.array(out_text)[idx_row[np.where(normalized_value_row > 1.2 * np.std(normalized_value_row))]]\n",
    "    if len(phon) > 0:\n",
    "        phon_list.append(\"\".join(phon))\n",
    "    else:\n",
    "        phon_list.append(\"$\")\n",
    "\n",
    "#Creates the g_p tupple list\n",
    "g_p = [(l, phon_list[i]) for i, l in enumerate(inp_text)]\n",
    "\n",
    "#Creates the final g_p mapping\n",
    "mapping = []\n",
    "for phon, letters in groupby(g_p, lambda x: x[1]):\n",
    "    graph = \"\".join([letter[0] for letter in letters])\n",
    "    mapping.append(graph + \"-\" + phon)\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gp_match(i, txt, pred, attentionMatrix, treshold = 0.5):\n",
    "    temp = []\n",
    "    previous = 0\n",
    "    if txt[i] != ['']:\n",
    "        if len(txt[i]) > 1 and len(pred[i]) > 1:\n",
    "            for j in range(len(pred[i])):\n",
    "                graph = \"\"\n",
    "                if j+1 < len(attentionMatrix[i]):            \n",
    "                    end = attentionMatrix[i][j+1].argmax()\n",
    "                    while ((previous < end or previous == attentionMatrix[i][j].argmax())\n",
    "                           and (attentionMatrix[i][j+1][previous] < treshold or attentionMatrix[i][j][previous] > treshold)):\n",
    "\n",
    "                        graph += txt[i][previous]\n",
    "                        previous +=1\n",
    "\n",
    "                else:           \n",
    "                    while (previous < len(attentionMatrix[i][j]) \n",
    "                           and (attentionMatrix[i][j][previous] > treshold\n",
    "                           or previous == attentionMatrix[i][j].argmax()+1\n",
    "                           or previous == attentionMatrix[i][j].argmax()+2\n",
    "                           or previous == attentionMatrix[i][j].argmax()+3)):\n",
    "                        graph += txt[i][previous]\n",
    "                        previous +=1                      \n",
    "                g = graph\n",
    "                p = pred[i][j]\n",
    "                gp = (\"-\").join([g,p])\n",
    "                temp.append(gp)\n",
    "\n",
    "                if g == \"\":\n",
    "                    shift(temp,j)\n",
    "\n",
    "        elif len(txt[i]) == 1:\n",
    "            g = txt[i][0]\n",
    "            p = pred[i][0]\n",
    "            gp = (\"-\").join([g,p])\n",
    "            temp.append(gp)\n",
    "\n",
    "        elif len(pred[i]) == 1:\n",
    "            g = (\"\").join(txt[i])\n",
    "            p = pred[i][0]\n",
    "            gp = (\"-\").join([g,p])\n",
    "            temp.append(gp)\n",
    "\n",
    "    return temp"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:1.5_cpu]",
   "language": "python",
   "name": "conda-env-1.5_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
