{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _load_model():\n",
    "    \"\"\"\n",
    "    Restore Checkpoint\n",
    "\n",
    "    :return: attention visualizer\n",
    "    \"\"\"\n",
    "    usr_dir.import_usr_dir('../submodule')\n",
    "\n",
    "    visualizer = visualization.AttentionVisualizer('transformer_base', 'transformer', \"data_dir\",\n",
    "                                                   'word_to_phonetic', beam_size=5)\n",
    "    tf.Variable(0, dtype=tf.int64, trainable=False, name='global_step')\n",
    "\n",
    "    sess = tf.train.MonitoredTrainingSession(\n",
    "        checkpoint_dir=\"../checkpoints/word_to_phonetic/transformer-transformer_base_single_gpu-fr-best_model\",\n",
    "        save_summaries_secs=0,\n",
    "    )\n",
    "\n",
    "    return sess, visualizer\n",
    "\n",
    "def _get_attention_matrices(input_word, sess, visualizer):\n",
    "    \"\"\"\n",
    "    Run model to get the attention matrices and phonetic text\n",
    "\n",
    "    :param input_word\n",
    "    :param sess: tf session\n",
    "    :param visualizer: attention visualizer\n",
    "    :return: inp_text, out_text(phonetic) and sum_all_layers\n",
    "    \"\"\"\n",
    "    _, inp_text, out_text, att_mats = visualizer.get_vis_data_from_string(sess, input_word)\n",
    "    inp_text = [str(c, 'Latin-1') for c in inp_text]  # Decodes Latin-1 because of Frenche and Spanish special chars\n",
    "    out_text = [str(c, 'Latin-1') for c in out_text]\n",
    "\n",
    "    # Removes both padding and \"end of sequence\" markers\n",
    "    inp_text = [v for v in inp_text if v != '<EOS>']\n",
    "    out_text = [v for v in out_text if v != '<EOS>' and v != '<pad>']\n",
    "\n",
    "    # Gets layes 0 and 4 of the \"inp_out\" matrices\n",
    "    att_matrices = np.array(attention._get_attention(inp_text, out_text, *att_mats)[\"inp_out\"][\"att\"])[np.array([0, 4]),\n",
    "                   :, :, :]\n",
    "\n",
    "    # Sum all attention heads\n",
    "    sum_all_head = np.sum(att_matrices, axis=1)\n",
    "\n",
    "    # Sum layers 0 and 4\n",
    "    sum_all_layers = _normalize(np.sum(sum_all_head, axis=0)[:len(out_text), :len(inp_text)])\n",
    "\n",
    "    return inp_text, out_text, sum_all_layers\n",
    "\n",
    "def _normalize(matrix):\n",
    "    \"\"\"\n",
    "        input: a numpy matrix\n",
    "        return: matrix with 0 mean and 1 std\n",
    "    \"\"\"\n",
    "    return (matrix - np.mean(matrix))/np.std(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/anaconda3/envs/1.5_cpu/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensor2tensor.visualization import attention\n",
    "from tensor2tensor.visualization import visualization\n",
    "from tensor2tensor.utils import usr_dir\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Importing user module submodule from path /home/olivier/Bureau/Transformer_test\n",
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_258_512.bottom\n",
      "INFO:tensorflow:Transforming 'targets' with symbol_modality_258_512.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "WARNING:tensorflow:From /home/olivier/anaconda3/envs/1.5_cpu/lib/python3.5/site-packages/tensor2tensor/layers/common_layers.py:513: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_258_512.top\n",
      "WARNING:tensorflow:From /home/olivier/anaconda3/envs/1.5_cpu/lib/python3.5/site-packages/tensor2tensor/layers/common_layers.py:1708: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Beam Decoding with beam size 5\n",
      "WARNING:tensorflow:From /home/olivier/anaconda3/envs/1.5_cpu/lib/python3.5/site-packages/tensor2tensor/utils/beam_search.py:93: calling reduce_logsumexp (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ../checkpoints/word_to_phonetic/transformer-transformer_base_single_gpu-fr-best_model/model.ckpt-9001\n",
      "INFO:tensorflow:Saving checkpoints for 9001 into ../checkpoints/word_to_phonetic/transformer-transformer_base_single_gpu-fr-best_model/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "sess, visualizer = _load_model()\n",
    "inp_text, out_text, sum_all_layers = _get_attention_matrices(\"test\", sess, visualizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_gp_match(txt, pred, attentionMatrix, treshold = 0.5):\n",
    "    temp = []\n",
    "    previous = 0\n",
    "    if txt != ['']:\n",
    "        if len(txt) > 1 and len(pred) > 1:\n",
    "            for j, p in enumerate(pred):\n",
    "                graph = \"\"\n",
    "                if j+1 < len(attentionMatrix):            \n",
    "                    end = attentionMatrix[j+1].argmax()\n",
    "                    while ((previous < end or previous == attentionMatrix[j].argmax())\n",
    "                           and (attentionMatrix[j+1][previous] < treshold or attentionMatrix[j][previous] > treshold)):\n",
    "\n",
    "                        graph += txt[previous]\n",
    "                        previous +=1\n",
    "\n",
    "                else:           \n",
    "                    while (previous < len(attentionMatrix[j]) \n",
    "                           and (attentionMatrix[j][previous] > treshold\n",
    "                           or previous == attentionMatrix[j].argmax()+1\n",
    "                           or previous == attentionMatrix[j].argmax()+2\n",
    "                           or previous == attentionMatrix[j].argmax()+3)):\n",
    "                        graph += txt[previous]\n",
    "                        previous +=1                      \n",
    "                g = graph\n",
    "                gp = (\"-\").join([g,p])\n",
    "                temp.append(gp)\n",
    "\n",
    "        elif len(txt) == 1:\n",
    "            g = txt[0]\n",
    "            p = pred[0]\n",
    "            gp = (\"-\").join([g,p])\n",
    "            temp.append(gp)\n",
    "\n",
    "        elif len(pred) == 1:\n",
    "            g = (\"\").join(txt)\n",
    "            p = pred[0]\n",
    "            gp = (\"-\").join([g,p])\n",
    "            temp.append(gp)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _mapping(inp_text, out_text, sum_all_layers):\n",
    "    # Base threshold\n",
    "    # fr : 0.75\n",
    "    # es : 0.4\n",
    "    if len(out_text) > 4:\n",
    "        threshold = 0.75\n",
    "    else:\n",
    "        threshold = 0\n",
    "\n",
    "    # While we have too many silent_letters detected\n",
    "    while (True):\n",
    "        # Gets the silent_letters indices\n",
    "        # We consider that a letter is silent if its attention value is below mean attention + threshold * std attention\n",
    "        silent_letters_idx = [i for i, idx in enumerate(np.argmax(sum_all_layers, axis=0))\n",
    "                              if sum_all_layers[idx, i] < np.mean(sum_all_layers[idx, :])\n",
    "                              + threshold * np.std(sum_all_layers[idx, :])]\n",
    "        # Reduces threshold if too many silent letters are detected\n",
    "        # Can happen in french when we have 3 lettres graphemes\n",
    "        if len(silent_letters_idx) > 1 / 3 * len(inp_text):\n",
    "            threshold -= 0.1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Creates the phoneme attribution list\n",
    "    phon_list = np.array(out_text)[np.argmax(sum_all_layers, axis=0)]\n",
    "    phon_list[silent_letters_idx] = \"#\"  # \"$\" is our encoding for silent letters\n",
    "    phon_list = phon_list.tolist()  # needed for the += just below\n",
    "\n",
    "    # Checks if all the phonemes are attributed and if they are only present the correct number of time in the list\n",
    "    # If not, the phoneme is concatenated to its most probable neighbor\n",
    "    # and the least probable phoneme is replaced by a silent letter (this can happen for small datasets)\n",
    "    for i, phon in enumerate(out_text):\n",
    "        if phon not in phon_list:\n",
    "            phon_list[np.argmax(sum_all_layers[i, :])] += phon\n",
    "\n",
    "    # test = np.where(np.array(phon_list) == phon)[0]\n",
    "    #     if len(test > 1):\n",
    "    #         phon_list[np.max(test)] = \"%\"\n",
    "\n",
    "    ##NOT WORKING PROPERLY\n",
    "\n",
    "    # Creates the g_p tupple list\n",
    "    g_p = [(l, phon_list[i]) for i, l in enumerate(inp_text)]\n",
    "\n",
    "    # Creates the final g_p mapping\n",
    "    mapping = []\n",
    "    for phon, letters in groupby(g_p, lambda x: x[1]):\n",
    "        graph = \"\".join([letter[0] for letter in letters])\n",
    "        mapping.append(graph + \"-\" + phon)\n",
    "\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unique_words(wordGp):\n",
    "    uniqueWordList = []\n",
    "    for word, pred, gpMatch, copy in wordGp:\n",
    "        if (word,pred) not in uniqueWordList:\n",
    "            uniqueWordList.append((word,pred))\n",
    "        else:\n",
    "            wordGp.remove((word, pred, gpMatch, copy))\n",
    "    return wordGp\n",
    "\n",
    "def generate_word_list(wordGp, gpProg):\n",
    "    \n",
    "    tempList = []\n",
    "    for i in range(len(gpProg)):\n",
    "        lesson = gpProg.loc[i]\n",
    "        \n",
    "        for word, pred, gpMatch, copy in wordGp[:]:\n",
    "            for gp in gpMatch[:]:\n",
    "                if gp == lesson[\"GP\"]:\n",
    "                    gpMatch.remove(gp)\n",
    "            if len(gpMatch) == 0:\n",
    "                tempList.append(((int(lesson[\"LESSON\"])),(\"\").join(word),(\".\").join(pred),(\".\").join(copy)))\n",
    "                wordGp.remove((word, pred, gpMatch, copy))\n",
    "    for word, pred, gpMatch, copy in wordGp[:]:\n",
    "        tempList.append((999,(\"\").join(word),(\".\").join(pred),(\".\").join(copy)))\n",
    "    \n",
    "    wordList = pd.DataFrame()\n",
    "    wordList = wordList.append(tempList,ignore_index=True)\n",
    "    wordList.columns = [[\"LESSON\", \"GRAPHEME\",\"PHONEME\", \"GPMATCH\"]]\n",
    "    return wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "gpProg = pd.read_csv('../api/word_to_phonetic/fr/files/gp_prog.csv', sep=\";\")\n",
    "gpProg.columns = [[\"GP\",\"LESSON\"]]\n",
    "gpProg = gpProg.loc[gpProg[\"GP\"].notnull()]\n",
    "\n",
    "corpus = ['test', 'avaient', 'danse']\n",
    "results = [_get_attention_matrices(word, sess, visualizer) for word in corpus]\n",
    "\n",
    "g_p_results = [_mapping(r[0], r[1], r[2]) for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"LESSON\":{\"0\":13,\"1\":999,\"2\":999},\"GRAPHEME\":{\"0\":\"suis\",\"1\":\"je\",\"2\":\"beau\"},\"PHONEME\":{\"0\":\"s.8.i\",\"1\":\"Z.*\",\"2\":\"b.O\"},\"GPMATCH\":{\"0\":\"s-s.u-8.i-i.s-#\",\"1\":\"j-Z.e-*\",\"2\":\"b-b.e-O.a-#.u-O\"}}'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "wordGp = list(zip(corpus, [r[1] for r in results], deepcopy(g_p_results),deepcopy(g_p_results)))\n",
    "wordGp = get_unique_words(wordGp)\n",
    "wordList = generate_word_list(wordGp, gpProg)\n",
    "wordList.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LESSON</th>\n",
       "      <th>GRAPHEME</th>\n",
       "      <th>PHONEME</th>\n",
       "      <th>GPMATCH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>suis</td>\n",
       "      <td>s.8.i</td>\n",
       "      <td>s-s.u-8.i-i.s-#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>999</td>\n",
       "      <td>je</td>\n",
       "      <td>Z.*</td>\n",
       "      <td>j-Z.e-*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>999</td>\n",
       "      <td>beau</td>\n",
       "      <td>b.O</td>\n",
       "      <td>b-b.e-O.a-#.u-O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LESSON GRAPHEME PHONEME          GPMATCH\n",
       "0      13     suis   s.8.i  s-s.u-8.i-i.s-#\n",
       "1     999       je     Z.*          j-Z.e-*\n",
       "2     999     beau     b.O  b-b.e-O.a-#.u-O"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../../test.txt\", 'r') as f:\n",
    "    corpus = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['je', 'suis', 'beau']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:1.5_cpu]",
   "language": "python",
   "name": "conda-env-1.5_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
