{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/anaconda3/envs/1.5_cpu/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import codecs\n",
    "from __future__ import print_function\n",
    "\n",
    "# Communication to TensorFlow server via gRPC\n",
    "from grpc.beta import implementations\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# TensorFlow serving stuff to send messages\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2\n",
    "from scipy.io import loadmat\n",
    "from copy import deepcopy\n",
    "from flask import send_file\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import random \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SERVER_NAME, SERVER_PORT = \"0.0.0.0\", \"9000\"\n",
    "\n",
    "VOCAB_BASED_LANGUAGES = [\"en\"]\n",
    "\n",
    "LANGUAGE = \"fr\"\n",
    "MODEL_NAME = \"w2p_\" + LANGUAGE\n",
    "\n",
    "# if LANGUAGE in VOCAB_BASED_LANGUAGES:\n",
    "#     VOCAB = \"../api/word_to_phonetic/\"+LANGUAGE+\"/vocab\"\n",
    "# else:\n",
    "VOCAB = None\n",
    "    \n",
    "DATASET = \"../data_dir/\"+LANGUAGE+\"_2/test_dataset.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _char_encode(input, padding_to = None):\n",
    "    \"\"\"\n",
    "    Transform txt input to int tokens\n",
    "    +2 is for special tokens [\"<EOS>\", \"<PAD>\"]\n",
    "    + [1] is to add end of sequence \"<EOS>\" token\n",
    "\n",
    "    :param input: String input\n",
    "    :return: [1, -1, 1, 1] Int array\n",
    "    \"\"\"\n",
    "    inp = [c + 2 for c in input.encode(\"Latin-1\")] + [1]\n",
    "    if padding_to:\n",
    "        for _ in range(padding_to - len(inp)):\n",
    "            inp += [0]\n",
    "    inp = np.reshape(inp, [1, -1, 1, 1])\n",
    "\n",
    "    return inp\n",
    "\n",
    "def _char_decode(input):\n",
    "    \"\"\"\n",
    "    Decode token ids to string and removes padding and eos\n",
    "\n",
    "    :param input: int array\n",
    "    :return: String\n",
    "    \"\"\"\n",
    "\n",
    "    return [chr(idx - 2) for idx in input if idx > 1]\n",
    "\n",
    "def _vocab_encode(input, vocab, padding_to = None):\n",
    "    with open(vocab, \"r\") as f:\n",
    "        vocab_arr = [l.strip()[1:-1] for l in f.readlines()]\n",
    "    \n",
    "    try :\n",
    "        inp = [np.where(np.array(vocab_arr) == (c.upper() + \"_\"))[0][0] for c in input] + [1]\n",
    "    except :\n",
    "        print(input)\n",
    "    if padding_to:\n",
    "        for _ in range(padding_to - len(inp)):\n",
    "            inp += [0]\n",
    "    inp = np.reshape(inp, [1, -1, 1, 1])\n",
    "    return inp\n",
    "\n",
    "def _vocab_decode(input, vocab):\n",
    "    with open(vocab, \"r\") as f:\n",
    "        vocab_arr = [l.strip()[1:-1] for l in f.readlines()]\n",
    "    return [vocab_arr[i][:-1] for i in input if i>1]\n",
    "\n",
    "def _create_translate_request(input, model_name):\n",
    "    \"\"\"\n",
    "    Creates translate request to TensorFlow serving server\n",
    "\n",
    "    :param input: Int array, token ids\n",
    "    :param model_name: Name of the model to serve\n",
    "    :return: PredictRequest object\n",
    "    \"\"\"\n",
    "    # create predict request\n",
    "    request = predict_pb2.PredictRequest()\n",
    "\n",
    "    # Call model to make phonetic translation\n",
    "    request.model_spec.name = model_name\n",
    "    request.model_spec.signature_name = \"get_phon\"\n",
    "    request.inputs['input'].CopyFrom(\n",
    "        tf.contrib.util.make_tensor_proto(input, dtype=tf.int32))\n",
    "\n",
    "    return request\n",
    "\n",
    "\n",
    "def _open_tf_server_channel(server_name, server_port):\n",
    "    \"\"\"\n",
    "    Opens channel to TensorFlow server for requests\n",
    "\n",
    "    :param server_name: String, server name (localhost, IP address)\n",
    "    :param server_port: String, server port\n",
    "    :return: Channel stub\n",
    "    \"\"\"\n",
    "    channel = implementations.insecure_channel(\n",
    "        server_name,\n",
    "        int(server_port))\n",
    "    stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n",
    "\n",
    "    return stub\n",
    "\n",
    "def _format_phon(phon):\n",
    "    values = phon.int_val\n",
    "    shape1 = phon.tensor_shape.dim[0].size\n",
    "    shape2 = phon.tensor_shape.dim[1].size\n",
    "\n",
    "    return np.reshape(values, [shape1, shape2])\n",
    "\n",
    "\n",
    "def _make_translation(input, model_name, stub):\n",
    "    translation_request = _create_translate_request(input, model_name)\n",
    "    translation_results = stub.Predict(translation_request, 60.0)\n",
    "    return _format_phon(translation_results.outputs[\"phon\"])\n",
    "\n",
    "\n",
    "def _make_translation_batch(batch_input, model_name, stub, vocab=None):\n",
    "    padding_to = len(max(batch_input, key=len)) + 1\n",
    "    \n",
    "    if not vocab:\n",
    "        batch_input_tokenized = np.stack([_char_encode(input, padding_to).squeeze(0) for input in batch_input], 0)\n",
    "\n",
    "    else:\n",
    "        batch_input_tokenized = np.stack([_vocab_encode(input, vocab, padding_to).squeeze(0) for input in batch_input], 0)\n",
    "\n",
    "    batch_phon_tokenized = _make_translation(batch_input_tokenized, model_name, stub)\n",
    "\n",
    "    if not vocab:\n",
    "        batch_phon = [\"\".join(_char_decode(phon_tokenized)) for phon_tokenized in batch_phon_tokenized]\n",
    "    else:\n",
    "        batch_phon = [\"\".join(_vocab_decode(phon_tokenized, vocab)) for phon_tokenized in batch_phon_tokenized]\n",
    "    \n",
    "    return batch_phon\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txt,phon = [], []\n",
    "with open(DATASET, \"r\") as f:\n",
    "    for l in f.readlines():\n",
    "        l = l.strip().split(\",\")\n",
    "        txt.append(l[0])\n",
    "        phon.append(l[1].replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[118]],\n",
       "\n",
       "        [[103]],\n",
       "\n",
       "        [[117]],\n",
       "\n",
       "        [[118]],\n",
       "\n",
       "        [[  1]]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_char_encode(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:39<00:00,  2.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# open channel to tensorflow server\n",
    "stub = _open_tf_server_channel(SERVER_NAME, SERVER_PORT)\n",
    "\n",
    "# get phonetic translation and attention matrices\n",
    "batch_size = 64\n",
    "wordCount = len(txt)\n",
    "n_batch = wordCount // batch_size\n",
    "\n",
    "pred_phon = []\n",
    "\n",
    "for idx_batch in tqdm(range(n_batch + 1)):\n",
    "    try:\n",
    "        batch = txt[idx_batch * batch_size:(idx_batch + 1) * batch_size]\n",
    "    except:\n",
    "        batch = txt[n_batch * batch_size:]\n",
    "    \n",
    "    batch_phon = _make_translation_batch(batch, MODEL_NAME, stub, VOCAB)\n",
    "    pred_phon.extend(batch_phon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twopenny // twopeni : t-xpxn-i\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "rand = random.randint(0,len(txt))\n",
    "print(\"{} // {} : {}\".format(txt[rand], pred_phon[rand], phon[rand]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 \n",
    "            deletions = current_row[j] + 1       \n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def per(s1,s2):\n",
    "    return levenshtein(s1,s2) / len(s1)\n",
    "\n",
    "def wer(s1,s2):\n",
    "    return int(s1!=s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_rates(s,p):\n",
    "    perS = 0\n",
    "    werS = 0\n",
    "    for i, _ in tqdm(enumerate(s)):\n",
    "        werS += wer(s[i],p[i])\n",
    "        perS += per(s[i],p[i])\n",
    "    return werS/len(s), perS/len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4002it [00:00, 32762.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER : 29.5852% ; PER : 6.5942%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rates = error_rates(phon,pred_phon)\n",
    "print(\"WER : {:.4%} ; PER : {:.4%}\".format(rates[0], rates[1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:1.5_cpu]",
   "language": "python",
   "name": "conda-env-1.5_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
