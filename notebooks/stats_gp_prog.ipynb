{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olivier/anaconda3/envs/1.5_cpu/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Communication to TensorFlow server via gRPC\n",
    "from grpc.beta import implementations\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# TensorFlow serving stuff to send messages\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2\n",
    "from scipy.io import loadmat\n",
    "from copy import deepcopy\n",
    "from flask import send_file\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "import random \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _char_encode(input, padding_to = None):\n",
    "    \"\"\"\n",
    "    Transform txt input to int tokens\n",
    "    +2 is for special tokens [\"<EOS>\", \"<PAD>\"]\n",
    "    + [1] is to add end of sequence \"<EOS>\" token\n",
    "\n",
    "    :param input: String input\n",
    "    :return: [1, -1, 1, 1] Int array\n",
    "    \"\"\"\n",
    "    inp = [c + 2 for c in input.encode(\"Latin-1\")] + [1]\n",
    "    if padding_to:\n",
    "        for _ in range(padding_to - len(inp)):\n",
    "            inp += [0]\n",
    "    inp = np.reshape(inp, [1, -1, 1, 1])\n",
    "\n",
    "    return inp\n",
    "\n",
    "def _char_decode(input):\n",
    "    \"\"\"\n",
    "    Decode token ids to string and removes padding and eos\n",
    "\n",
    "    :param input: int array\n",
    "    :return: String\n",
    "    \"\"\"\n",
    "\n",
    "    return [chr(idx - 2) for idx in input if idx > 1]\n",
    "\n",
    "def _vocab_encode(input, vocab, padding_to = None):\n",
    "    with open(vocab, \"r\") as f:\n",
    "        vocab_arr = [l.strip()[1:-1] for l in f.readlines()]\n",
    "    \n",
    "    try :\n",
    "        inp = [np.where(np.array(vocab_arr) == (c.upper() + \"_\"))[0][0] for c in input] + [1]\n",
    "    except :\n",
    "        print(input)\n",
    "    if padding_to:\n",
    "        for _ in range(padding_to - len(inp)):\n",
    "            inp += [0]\n",
    "    inp = np.reshape(inp, [1, -1, 1, 1])\n",
    "    return inp\n",
    "\n",
    "def _vocab_decode(input, vocab):\n",
    "    with open(vocab, \"r\") as f:\n",
    "        vocab_arr = [l.strip()[1:-1] for l in f.readlines()]\n",
    "    return [vocab_arr[i][:-1] for i in input if i>1]\n",
    "\n",
    "def _create_translate_request(input, model_name):\n",
    "    \"\"\"\n",
    "    Creates translate request to TensorFlow serving server\n",
    "\n",
    "    :param input: Int array, token ids\n",
    "    :param model_name: Name of the model to serve\n",
    "    :return: PredictRequest object\n",
    "    \"\"\"\n",
    "    # create predict request\n",
    "    request = predict_pb2.PredictRequest()\n",
    "\n",
    "    # Call model to make phonetic translation\n",
    "    request.model_spec.name = model_name\n",
    "    request.model_spec.signature_name = \"get_phon\"\n",
    "    request.inputs['input'].CopyFrom(\n",
    "        tf.contrib.util.make_tensor_proto(input, dtype=tf.int32))\n",
    "\n",
    "    return request\n",
    "\n",
    "\n",
    "def _create_att_mats_request(input, phon, model_name):\n",
    "    \"\"\"\n",
    "    Creates attention matrices request to TensorFlow serving server\n",
    "\n",
    "    :param input: Int array, token ids\n",
    "    :param phon: Int array, token ids, result of the phonetic translation\n",
    "    :param model_name: name of the model to serve\n",
    "    :return: PredictRequest object\n",
    "    \"\"\"\n",
    "\n",
    "    # create predict request\n",
    "    request = predict_pb2.PredictRequest()\n",
    "\n",
    "    # Call model to get the correct attention matrices\n",
    "    request.model_spec.name = model_name\n",
    "    request.model_spec.signature_name = \"get_att_mats\"\n",
    "    request.inputs[\"input\"].CopyFrom(\n",
    "        tf.contrib.util.make_tensor_proto(input, dtype=tf.int32))\n",
    "    request.inputs[\"phon\"].CopyFrom(\n",
    "        tf.contrib.util.make_tensor_proto(phon, dtype=tf.int32))\n",
    "\n",
    "    return request\n",
    "\n",
    "def _open_tf_server_channel(server_name, server_port):\n",
    "    \"\"\"\n",
    "    Opens channel to TensorFlow server for requests\n",
    "\n",
    "    :param server_name: String, server name (localhost, IP address)\n",
    "    :param server_port: String, server port\n",
    "    :return: Channel stub\n",
    "    \"\"\"\n",
    "    channel = implementations.insecure_channel(\n",
    "        server_name,\n",
    "        int(server_port))\n",
    "    stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n",
    "\n",
    "    return stub\n",
    "\n",
    "def _format_phon(phon):\n",
    "    values = phon.int_val\n",
    "    shape1 = phon.tensor_shape.dim[0].size\n",
    "    shape2 = phon.tensor_shape.dim[1].size\n",
    "\n",
    "    return np.reshape(values, [shape1, shape2])\n",
    "\n",
    "def _format_att_mat(att_mat):\n",
    "    values = att_mat.float_val\n",
    "    shape1 = att_mat.tensor_shape.dim[0].size\n",
    "    shape2 = att_mat.tensor_shape.dim[1].size\n",
    "    \n",
    "    if len(att_mat.tensor_shape.dim) == 2:\n",
    "        return np.reshape(values, [shape1, shape2])\n",
    "    elif len(att_mat.tensor_shape.dim) == 3:\n",
    "        shape3 = att_mat.tensor_shape.dim[2].size\n",
    "        return np.reshape(values, [shape1, shape2, shape3])\n",
    "\n",
    "\n",
    "def _make_translation(input, model_name, stub):\n",
    "    translation_request = _create_translate_request(input, model_name)\n",
    "    translation_results = stub.Predict(translation_request, 60.0)\n",
    "    return _format_phon(translation_results.outputs[\"phon\"])\n",
    "\n",
    "\n",
    "def _make_att_matrices(input, phon, model_name, stub):\n",
    "    att_mats_request = _create_att_mats_request(input, phon, model_name)\n",
    "    att_mats_results = stub.Predict(att_mats_request, 60.0)\n",
    "\n",
    "    att_mats = []\n",
    "    att_mats.append(_format_att_mat(att_mats_results.outputs[\"att_mat_inp_out_layer_0\"]))\n",
    "    att_mats.append(_format_att_mat(att_mats_results.outputs[\"att_mat_inp_out_layer_4\"]))\n",
    "    att_mats.append(_format_att_mat(att_mats_results.outputs[\"att_mat_inp_out_layer_5\"]))\n",
    "\n",
    "    return np.array(att_mats)\n",
    "\n",
    "\n",
    "def _make_prediction(input, model_name, stub, vocab=None):\n",
    "    if not vocab:\n",
    "        input_tokenized = _char_encode(input)\n",
    "    else:\n",
    "        input_tokenized = _vocab_encode(input, vocab)\n",
    "\n",
    "    phon_tokenized = np.squeeze(_make_translation(input_tokenized, model_name, stub))\n",
    "\n",
    "    if not vocab:\n",
    "        phon = _char_decode(phon_tokenized)\n",
    "    else:\n",
    "        phon = _vocab_decode(phon_tokenized, vocab)\n",
    "\n",
    "    att_mats = _make_att_matrices(input_tokenized, np.reshape(phon_tokenized, [1, -1, 1, 1]), model_name, stub)\n",
    "\n",
    "    sum_all_layers = _normalize(np.sum(np.array(att_mats), axis=0)[:len(phon), :len(input)])\n",
    "\n",
    "    return phon, sum_all_layers\n",
    "\n",
    "def _make_prediction_batch(batch_input, model_name, stub, vocab=None):\n",
    "    padding_to = len(max(batch_input, key=len)) + 1\n",
    "    \n",
    "    if not vocab:\n",
    "        batch_input_tokenized = np.stack([_char_encode(input, padding_to).squeeze(0) for input in batch_input], 0)\n",
    "\n",
    "    else:\n",
    "        batch_input_tokenized = np.stack([_vocab_encode(input, vocab, padding_to).squeeze(0) for input in batch_input], 0)\n",
    "\n",
    "    batch_phon_tokenized = _make_translation(batch_input_tokenized, model_name, stub)\n",
    "\n",
    "    if not vocab:\n",
    "        batch_phon = [_char_decode(phon_tokenized) for phon_tokenized in batch_phon_tokenized]\n",
    "    else:\n",
    "        batch_phon = [_vocab_decode(phon_tokenized, vocab) for phon_tokenized in batch_phon_tokenized]\n",
    "    \n",
    "    batch_att_mats = _make_att_matrices(batch_input_tokenized, np.reshape(batch_phon_tokenized, [len(batch_input), -1, 1, 1]), model_name, stub)\n",
    "\n",
    "    batch_sum_all_layers = _normalize(np.sum(np.array(batch_att_mats), axis=0))\n",
    "\n",
    "    return batch_phon, batch_sum_all_layers\n",
    "\n",
    "\n",
    "def g2p_mapping_once(input, model_name, vocab=None):\n",
    "    \"\"\"\n",
    "    Predict the phonetic translation of a word using a Transformer model\n",
    "\n",
    "    :param input: String, word\n",
    "    :param model_name: Name of the model to serve\n",
    "    :return: Array[3], [0] input text, [1] phonetic translation, [2] mapping\n",
    "    \"\"\"\n",
    "\n",
    "    # get TensorFlow server connection parameters\n",
    "    server_name, server_port = \"0.0.0.0\", \"9000\"\n",
    "\n",
    "    # open channel to tensorflow server\n",
    "    stub = _open_tf_server_channel(server_name, server_port)\n",
    "\n",
    "    # get phonetic translation and attention matrices\n",
    "    phon, sum_all_layers = _make_prediction(input, model_name, stub, vocab)\n",
    "\n",
    "    # make prediction\n",
    "\n",
    "    return _mapping(input, phon, sum_all_layers)\n",
    "\n",
    "def g2p_mapping_batch(batch_input, model_name, vocab=None):\n",
    "    \"\"\"\n",
    "    Predict the phonetic translation of a word using a Transformer model\n",
    "\n",
    "    :param input: String, word\n",
    "    :param model_name: Name of the model to serve\n",
    "    :return: Array[3], [0] input text, [1] phonetic translation, [2] mapping\n",
    "    \"\"\"\n",
    "\n",
    "    # get TensorFlow server connection parameters\n",
    "    server_name, server_port = \"0.0.0.0\", \"9000\"\n",
    "\n",
    "    # open channel to tensorflow server\n",
    "    stub = _open_tf_server_channel(server_name, server_port)\n",
    "\n",
    "    # get phonetic translation and attention matrices\n",
    "    batch_phon, batch_sum_all_layers = _make_prediction_batch(batch_input, model_name, stub, vocab)\n",
    "\n",
    "    # make prediction\n",
    "    return [_mapping(input, batch_phon[idx], batch_sum_all_layers[idx, :len(batch_phon[idx]), :len(input)]) for idx, input in enumerate(batch_input)]\n",
    "\n",
    "def g2p_mapping_file(corpus, progression, model_name, vocab=None):\n",
    "    gpProg = _load_gp_prog(progression)\n",
    "\n",
    "    # get TensorFlow server connection parameters\n",
    "    server_name, server_port = \"0.0.0.0\", \"9000\"\n",
    "\n",
    "    # open channel to tensorflow server\n",
    "    stub = _open_tf_server_channel(server_name, server_port)\n",
    "\n",
    "    # get phonetic translation and attention matrices\n",
    "    phon_results = []\n",
    "    g_p_results = []\n",
    "    for word in corpus:\n",
    "        phon, sum_all_layers = _make_prediction(word, model_name, stub, vocab)\n",
    "        phon_results.append(phon)\n",
    "        g_p_results.append(_mapping(word, phon, sum_all_layers)[2])\n",
    "\n",
    "    wordGp = list(zip(corpus, phon_results, deepcopy(g_p_results), deepcopy(g_p_results)))\n",
    "    wordGp = _get_unique_words(wordGp)\n",
    "    wordList = _generate_word_list(wordGp, gpProg)\n",
    "\n",
    "    path = os.path.join(settings.FR_FILES, \"results.csv\")\n",
    "    wordList.to_csv(path, encoding=\"UTF-8\")\n",
    "    return send_file(path)\n",
    "\n",
    "def _normalize(matrix):\n",
    "    \"\"\"\n",
    "        input: a numpy matrix\n",
    "        return: matrix with 0 mean and 1 std\n",
    "    \"\"\"\n",
    "    return (matrix - np.mean(matrix)) / (np.std(matrix) + 1e-10)\n",
    "\n",
    "\n",
    "def _mapping(inp_text, out_text, sum_all_layers):\n",
    "    # Base threshold\n",
    "    # fr : 0.75\n",
    "    # es : 0.4\n",
    "    if len(out_text) > 4:\n",
    "        threshold = 0.4\n",
    "    else:\n",
    "        threshold = 0\n",
    "\n",
    "    # While we have too many silent_letters detected\n",
    "    while (True):\n",
    "        # Gets the silent_letters indices\n",
    "        # We consider that a letter is silent if its attention value is below mean attention + threshold * std attention\n",
    "        silent_letters_idx = [i for i, idx in enumerate(np.argmax(sum_all_layers, axis=0))\n",
    "                              if sum_all_layers[idx, i] < np.mean(sum_all_layers[idx, :])\n",
    "                              + threshold * np.std(sum_all_layers[idx, :])]\n",
    "        # Reduces threshold if too many silent letters are detected\n",
    "        # Can happen in french when we have 3 lettres graphemes\n",
    "        if len(silent_letters_idx) > 1 / 3 * len(inp_text):\n",
    "            threshold -= 0.1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Creates the phoneme attribution list\n",
    "    phon_list = np.array(out_text)[np.argmax(sum_all_layers, axis=0)]\n",
    "    phon_list[silent_letters_idx] = \"#\"  # \"#\" is our encoding for silent letters\n",
    "    phon_list = phon_list.tolist()  # needed for the += just below\n",
    "\n",
    "    # Checks if all the phonemes are attributed and if they are only present the correct number of time in the list\n",
    "    # If not, the phoneme is concatenated to its most probable neighbor\n",
    "    # and the least probable phoneme is replaced by a silent letter (this can happen for small datasets)\n",
    "    discard_next = False\n",
    "    for i, phon in enumerate(out_text):\n",
    "        if phon not in phon_list and not discard_next:\n",
    "            probable_idx = np.argmax(sum_all_layers[i, :])\n",
    "            if probable_idx == (i+2):\n",
    "                phon_list[probable_idx] = phon + phon_list[probable_idx]\n",
    "                discard_next = True\n",
    "            else:\n",
    "                phon_list[np.argmax(sum_all_layers[i, :])] += phon\n",
    "        elif discard_next:\n",
    "            discard_next = False\n",
    "\n",
    "    # test = np.where(np.array(phon_list) == phon)[0]\n",
    "    #     if len(test > 1):\n",
    "    #         phon_list[np.max(test)] = \"%\"\n",
    "\n",
    "    ##NOT WORKING PROPERLY\n",
    "\n",
    "    # Creates the g_p tupple list\n",
    "    g_p = [(l, phon_list[i]) for i, l in enumerate(inp_text)]\n",
    "\n",
    "    # Creates the final g_p mapping\n",
    "    mapping = []\n",
    "    for phon, letters in groupby(g_p, lambda x: x[1]):\n",
    "        graph = \"\".join([letter[0] for letter in letters])\n",
    "        mapping.append(graph + \"~\" + phon)\n",
    "\n",
    "    return [\"\".join(inp_text), \" \".join(out_text), mapping]\n",
    "\n",
    "def _dic_add(value,dic):\n",
    "    if value not in dic:\n",
    "        dic[value] = 1\n",
    "    else:\n",
    "        dic[value] +=1\n",
    "\n",
    "def _dic_add_txt(value,dic,txt):\n",
    "    if value not in dic:\n",
    "        dic[value] = txt\n",
    "    else:\n",
    "        if len(txt) < 5  and random.randint(0,1)>0.9:\n",
    "            dic[value] = txt\n",
    "\n",
    "def _delete_outliner(gp,gpCount):\n",
    "    if gpCountDic[gp]/gpCount < 0.001:\n",
    "        g,p = gp.split(\"~\")\n",
    "        gpCount -= gpCountDic[gp]\n",
    "        gCountDic[g] -= gpCountDic[gp]\n",
    "        pCountDic[p] -= gpCountDic[gp]       \n",
    "        \n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WRITEDOWN',\n",
       " 'R AY1 T IH0 D AW2 N',\n",
       " ['WR~R', 'I~AY1', 'T~T', 'E~AY1', 'D~IH0D', 'OW~AW2', 'N~N']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g2p_mapping_once(\"WRITEDOWN\", \"w2p_en\", \"../api/word_to_phonetic/en/en_vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"../data_dir/en/dataset.csv\", \"r\") as f:\n",
    "    corpus = [l.strip().split(\";\")[0] for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 220/2091 [01:33<13:18,  2.34it/s]"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "wordCount = len(corpus)\n",
    "\n",
    "results = []\n",
    "\n",
    "n_batch = wordCount // batch_size\n",
    "for idx_batch in tqdm(range(n_batch + 1)):\n",
    "    try :\n",
    "        batch = corpus[idx_batch*batch_size:(idx_batch+1)*batch_size]\n",
    "    except :\n",
    "        batch = corpus[n_batch*batch_size:]\n",
    "    results.extend(g2p_mapping_batch(batch, \"w2p_en\", \"../api/word_to_phonetic/en/en_vocab\"))\n",
    "\n",
    "\n",
    "# for word in tqdm(corpus):\n",
    "#     results = g2p_mapping_once(word, \"w2p_en\", \"../api/word_to_phonetic/en/en_vocab\")\n",
    "#     temp = []\n",
    "#     for gp in results[2]:\n",
    "#         g,p = gp.split(\"~\")\n",
    "#         if gp not in gpList:\n",
    "#             gpList.append(gp)\n",
    "#         _dic_add(gp,gpCountDic)\n",
    "#         _dic_add(g,gCountDic)\n",
    "#         _dic_add(p,pCountDic)\n",
    "#         if gp not in temp:\n",
    "#             _dic_add(gp, wordCountDic)\n",
    "\n",
    "#         _dic_add_txt(gp,exampleDic,results[0])\n",
    "#         temp.append(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpList = []\n",
    "gpCountDic = {}\n",
    "gCountDic = {}\n",
    "pCountDic = {}\n",
    "wordCountDic = {}\n",
    "exampleDic = {}\n",
    "gpMatch = []\n",
    "gpCount = 0\n",
    "wordCount = len(corpus)\n",
    "\n",
    "for r in results:\n",
    "    temp = []\n",
    "    for gp in r[2]:\n",
    "        g,p = gp.split(\"~\")\n",
    "        if gp not in gpList:\n",
    "            gpList.append(gp)\n",
    "        _dic_add(gp,gpCountDic)\n",
    "        _dic_add(g,gCountDic)\n",
    "        _dic_add(p,pCountDic)\n",
    "        if gp not in temp:\n",
    "            _dic_add(gp, wordCountDic)\n",
    "\n",
    "        _dic_add_txt(gp,exampleDic,r)\n",
    "        temp.append(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpCount = sum(gpCountDic.values())\n",
    "tupList = []\n",
    "for i in range(len(gpList)):\n",
    "    gp = gpList[i]\n",
    "    g,p = gp.split(\"~\")\n",
    "    tup = (gp, wordCountDic[gp], wordCountDic[gp]/wordCount, gpCountDic[gp], gpCountDic[gp]/gpCount,\n",
    "              gpCountDic[gp]/gCountDic[g], gpCountDic[gp]/pCountDic[p],exampleDic[gp])\n",
    "    tupList.append(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = df.append(tupList,ignore_index=True)\n",
    "df.columns = [[\"GP\", \"WORDS COUNT\",\"WORD FREQ\", \"GP COUNT\", \n",
    "                              \"GP FREQ\", \"G CONSISTENCY\", \"P CONSISTENCY\", \"EXAMPLE\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_word_count(weights):\n",
    "    scores = np.dot(weights, np.transpose(df[[\"WORD FREQ\", \"GP FREQ\", \"G CONSISTENCY\", \"P CONSISTENCY\"]]))\n",
    "    df[\"SCORE\"] = scores\n",
    "    df.sort_values([\"SCORE\"],ascending=False, inplace=True)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    top = df[0:80]\n",
    "    \n",
    "    cn = 0\n",
    "    for r in results:\n",
    "        discard = False\n",
    "        for gp in r[2]:\n",
    "            if gp not in top[\"GP\"].values:\n",
    "                discard = True\n",
    "        if discard:\n",
    "            cn +=1\n",
    "    return cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = [50,10,0,40]\n",
    "get_word_count(weights)/len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:1.5_cpu]",
   "language": "python",
   "name": "conda-env-1.5_cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
